{
    "analyzer": {
        "sidebar": {
            "goBack": "Go Back",
            "overview": "Overview",
            "architectureDiagram": "Architecture Diagram",
            "infrastructureCode": "Infrastructure Code",
            "s3Module": "S3 Module",
            "iamModule": "IAM Module",
            "lambdaModule": "Lambda Module",
            "cloudtrailModule": "CloudTrail Module",
            "eventBridgeModule": "EventBridge Module",
            "terraformMainFile": "Terraform Main File",
            "lambdaAggregatorFunction": "Lambda Aggregator Function",
            "lambdaAnalyzerFunction": "Lambda Analyzer Function"
        },
        "overview": {
            "title": "Serverless Logs Analyzer - Overview",
            "description": "This project demonstrates the use of AWS to generate and analyze logs. It was designed to provide automated insights into AWS account activities through a serverless architecture that monitors, processes, and analyzes CloudTrail logs in real-time. The system automatically captures all AWS API calls and management events via CloudTrail, aggregates daily log files for efficient processing, and generates comprehensive statistical reports about resource usage patterns, security events, and operational activities. The solution uses Lambda functions, S3 storage, and EventBridge to create a cost-effective, scalable log analysis system that requires no infrastructure management while providing comprehensive visibility into AWS account activities.",
            "githubLink": "Github repository",
            "dashboardLink1": "To see the dashboard created with the processed logs, go to the dashboard page",
            "dashboardLink2": "here"
        },
        "architectureDiagram": {
            "title": "Architecture Diagram",
            "description": "The architecture diagram below shows the infrastructure, which includes:",
            "cloudtrail": "used to track all actions performed in the AWS account.",
            "eventbridge": "used to trigger the AWS Lambda function when a new log is created.",
            "lambda": "used to analyze the logs and store the results in Amazon S3.",
            "s3": "used to store the logs and the results.",
            "figcaption": "Diagram created with Lucidchart: "
        },
        "infrastructureCode": {
            "title": "Infrastructure Code",
            "description": "The infrastructure is implemented through a modular architecture using Terraform, which provisions the following AWS components organized into 5 main modules.",
            "modulesTitle": "Modules",
            "s3Module1": "S3 Module: ",
            "s3Module2": "This module is responsible for creating the storage infrastructure with two S3 buckets. It configures the input bucket to receive CloudTrail logs and the output bucket for processed insights. Both buckets are secured with versioning enabled, AES256 encryption, and complete public access blocking to ensure data protection and compliance.",
            "iamModule1": "IAM Module: ",
            "iamModule2": "The Identity and Access Management (IAM) module manages security permissions for the Lambda functions. It creates a dedicated execution role with precise permissions for S3 operations (read, write, delete) on both buckets and CloudWatch logging capabilities, following the principle of least privilege for enhanced security.",
            "lambdaModule1": "Lambda Module: ",
            "lambdaModule2": "This module handles the serverless compute functions that process the logs. It deploys two Python 3.11 Lambda functions - the Analyzer for processing aggregated logs and generating insights, and the Aggregator for consolidating daily log files. The module also configures S3 event triggers and proper permissions for seamless integration.",
            "cloudtrailModule1": "CloudTrail Module: ",
            "cloudtrailModule2": " The CloudTrail module sets up comprehensive AWS account auditing and logging. It configures a multi-region trail that captures management events, global service events, and S3 data events, automatically delivering all logs to the input S3 bucket with appropriate bucket policies for secure log storage.",
            "eventBridgeModule1": "EventBridge Module: ",
            "eventBridgeModule2": "This module manages the automated scheduling system for log processing. It creates a CloudWatch Event Rule with a cron expression to trigger the Lambda Aggregator function every 12 hours, ensuring regular log consolidation and processing without manual intervention.",
            "moduleFilesDescription1": "Inside each module, there is a ",
            "moduleFilesDescription2": "file that defines the resources, a ",
            "moduleFilesDescription3": "file that defines the variables, and an ",
            "moduleFilesDescription4": "file that defines the outputs, when needed."
        },
        "s3Module": {
            "title": "S3 Module",
            "description": "This module is designed to create the storage infrastructure with two S3 buckets, one for input logs and one for output insights. The input bucket receives CloudTrail logs automatically, while the output bucket stores the processed JSON and CSV reports. Both buckets are configured with versioning enabled for change tracking, AES256 server-side encryption for data protection, and complete public access blocking to ensure security compliance.",
            "featuresTitle": "Features",
            "s3DescriptionTitle": "S3 Buckets: ",
            "s3Description": "Creates two S3 buckets - an input bucket for receiving CloudTrail logs and an output bucket for storing processed insights and reports.",
            "versioningDescriptionTitle": "Versioning: ",
            "versioningDescription": "Enables versioning on both buckets to maintain object history and provide rollback capabilities for data protection.",
            "encryptionDescriptionTitle": "Encryption: ",
            "encryptionDescription": "Configures AES256 encryption by default on all objects stored in both buckets to ensure data security at rest.",
            "publicAccessBlockDescriptionTitle": "Public Access Blocking: ",
            "publicAccessBlockDescription": "Implements comprehensive public access restrictions on both buckets, blocking all forms of public access including ACLs and bucket policies to prevent unauthorized access.",
            "resourcesTitle ": "Resources",
            "buckets": {
                "bucketDescription": "The unique name identifier for the S3 bucket within AWS.",
                "forceDestroyDescription": "Whether to allow deletion of the bucket even when it contains objects.",
                "tagsDescription": "Assigns a name tag to the bucket for identification and resource management purposes.   "
            },
            "versioning": {
                "bucket": "The S3 bucket to enable versioning for.",
                "versoningConfiguration": "Configuration block for versioning settings."
            },
            "encryption": {
                "bucket": "The S3 bucket to enable encryption for.",
                "rule": "Configuration block for encryption settings.",
                "applyServerSideEncryptionByDefault": "Configuration block for encryption settings.",
                "sseAlgorithm": "The server-side encryption algorithm to use."
            },
            "publicAccessBlock": {
                "bucket": "The S3 bucket to enable public access block for.",
                "blockPublicAcls": "Whether to block public ACLs for this bucket.",
                "blockPublicPolicy": "Whether to block public bucket policies for this bucket.",
                "ignorePublicAcls": "Whether to ignore public ACLs for this bucket.",
                "restrictPublicBuckets": "Whether to restrict public bucket policies for this bucket."
            }
        },
        "iamModule": {
            "title": "IAM Module",
            "description": "This module is designed to create the security and access management infrastructure for the Lambda functions. It establishes a dedicated IAM role with precisely scoped permissions following the principle of least privilege. The role allows Lambda functions to interact with S3 buckets for log processing and CloudWatch for logging, while restricting access to only the necessary resources and actions required for the log analysis workflow.",
            "featuresTitle": "Features",
            "iamRoleTitle": "IAM role: ",
            "iamRoleDescription": "Creates a dedicated execution role specifically for Lambda functions with a trust policy that allows only the Lambda service to assume the role",
            "iamPolicyTitle": "IAM policy: ",
            "iamPolicyDescription": "Attaches a policy to the role that grants the necessary permissions for the Lambda function to access the S3 bucket and CloudWatch logs.",
            "iamRole": {
                "name": "The name of the IAM role.",
                "assumeRolePolicy": "The policy that allows the Lambda service to assume the role. It defines which services are allowed to assume the role, in this case, only the Lambda service."
            },
            "iamPolicy": {
                "name": "The name of the IAM policy.",
                "role": "The IAM role to attach the policy to.",
                "policy": "The policy to attach to the IAM role. It grants the necessary permissions for the Lambda function to access the S3 bucket and CloudWatch logs."
            }
        },
        "lambdaModule": {
            "title": "Lambda Module",
            "description": "This module is designed to create the serverless compute infrastructure for log processing and analysis. It establishes two specialized Lambda functions that handle different aspects of the log analysis workflow - one for aggregating daily logs and another for processing aggregated logs to generate insights. The module also configures the necessary triggers, permissions, and integrations to enable automatic execution based on S3 events and scheduled intervals.",
            "featuresTitle": "Features",
            "lambdaAggregatorFunctionTitle": "Lambda Aggregator Function: ",
            "lambdaAggregatorFunctionDescription": "Deploys a Python 3.11 Lambda function that consolidates multiple daily log files into single aggregated files, designed to be triggered by EventBridge scheduling.",
            "lambdaAnalyzerFunctionTitle": "Lambda Analyzer Function: ",
            "lambdaAnalyzerFunctionDescription": " Creates a Python 3.11 Lambda function that processes aggregated CloudTrail logs and generates statistical insights, triggered automatically by S3 ObjectCreated events.",
            "lambdaPermissionsTitle": "Lambda Permissions: ",
            "lambdaPermissionsDescription": "Establishes proper permissions allowing S3 service to invoke the Lambda functions while maintaining security boundaries.",
            "s3EventTriggerTitle": "S3 Event Triggers: ",
            "eventBridgeTriggerDescription": " Configures S3 bucket notifications to automatically invoke the analyzer function whenever new log files are uploaded to the input bucket.",
            "lambdaFunctions": {
                "functionName": "The name of the Lambda function.",
                "role": "The role that the Lambda function assumes when it executes, providing necessary permissions.",
                "handler": "The entry point for the Lambda function, specifying the file and function name to execute.",
                "runtime": "The runtime environment for the Lambda function.",
                "filename": "The path to the deployment package (ZIP file) containing the Lambda function code.",
                "sourceCodeHash": " The hash of the source code for the Lambda function.",
                "timeout": "The maximum execution time in seconds for the Lambda function.",
                "environment": "The environment variables for the Lambda function.",
                "tags": "The tags of the Lambda function."
            },
            "lambdaPermissions": {
                "statement_id": "The unique identifier for the permission.",
                "action": "he action to allow the Lambda function to perform.",
                "principal": "The service or entity that is allowed to invoke the Lambda function.",
                "function_name": "The name of the Lambda function that is allowed to be invoked by the principal.",
                "source_arn": "The ARN of the resource that is allowed to invoke the Lambda function."
            },
            "s3EventTrigger": {
                "bucket": "The S3 bucket to configure the notification for.",
                "action": " The action to allow the Lambda function to perform.",
                "depends_on": "Ensures that the Lambda function exists before the S3 bucket notification is configured.",
                "lambda_function": "The Lambda function to be triggered when the event occurs and the event type that triggers the Lambda function."
            }
        },
        "cloudtrailModule": {
            "title": "CloudTrail Module",
            "description": "This module is designed to create comprehensive AWS account auditing and logging infrastructure. It establishes a multi-region CloudTrail that captures all management events, global service events, and S3 data events across the entire AWS account. The module automatically delivers all captured logs to the designated S3 input bucket and configures the necessary bucket policies to enable secure log storage while maintaining proper access controls for the CloudTrail service.",
            "featuresTitle": "Features",
            "cloudtrailTrailConfigurationTitle": "CloudTrail Configuration: ",
            "cloudtrailTraiConfigurationlDescription": "Creates a multi-region trail that monitors AWS account activities across all regions, including global services like IAM, Route53, and CloudFront to provide complete visibility. The CloudTrail also captures all management plane operations such as resource creation, deletion, and configuration changes for comprehensive account auditing.",
            "s3IntegrationTitle": "S3 Integration: ",
            "s3IntegrationDescription": "Automatically delivers all captured logs to the input S3 bucket, and establishes secure S3 bucket policies that grant CloudTrail service the minimum necessary permissions to write logs and read bucket ACLs while maintaining security boundaries.",
            "cloudtrailConfiguration": {
                "name": "The name of the CloudTrail.",
                "s3BucketName": "The name of the S3 bucket to store the CloudTrail logs.",
                "isMultiRegionTrail": "Whether the trail captures events from all AWS regions or just where it's created.",
                "includeGlobalServiceEvents": "Whether to include events from global AWS services like IAM and STS.",
                "enableLogging": "Whether to enable logging.",
                "eventSelector": "Configuration block that defines which types of events the trail should capture.",
                "dependsOn": "Ensures the CloudTrail is created only after the specified dependencies are established."
            },
            "s3BucketPolicy": {
                "bucket": "The S3 bucket to configure the policy for.",
                "policy": "The JSON-encoded policy document that defines the permissions and access controls for the bucket, specifying who can perform what actions on the bucket and its objects."
            }
        },
        "eventBridgeModule": {
            "title": "EventBridge Module",
            "description": "This module is designed to create automated scheduling infrastructure for periodic log processing tasks. It establishes a CloudWatch Event Rule with cron-based scheduling that automatically triggers the Lambda Aggregator function every 12 hours to consolidate daily CloudTrail logs. The module configures the necessary event targets, permissions, and payload delivery to enable seamless integration between the scheduling service and the serverless compute functions..",
            "featuresTitle": "Features",
            "cloudwatchEventRuleTitle": "CloudWatch Event Rule: ",
            "cloudwatchEventRuleDescription": "Creates an EventBridge rule that automatically triggers the Lambda analyzer function whenever new log files are uploaded to the input S3 bucket.",
            "lambdaTargetTitle": "Lambda Target Configuration: ",
            "lambdaTargetDescription": "Establishes the Lambda Aggregator function as the target for the scheduled events, with proper payload configuration including account ID for contextual processing.",
            "eventBridgePermissionsTitle": "EventBridge Permissions: ",
            "eventBridgePermissionsDescription": "Configures Lambda permissions to allow the EventBridge service to invoke the aggregator function while maintaining security boundaries and preventing unauthorized access.",
            "cloudwatchEventRule": {
                "name": "The unique name identifier for the EventBridge rule within the AWS account.",
                "description": "The description of the EventBridge rule.",
                "scheduleExpression": "The cron or rate expression that defines when the rule should trigger, supporting both cron syntax and rate-based scheduling formats."
            },
            "lambdaTarget": {
                "rule": "The name of the EventBridge rule to which the target is attached.",
                "targetId": "The unique identifier for the target within the EventBridge rule.",
                "arn": "The Amazon Resource Name (ARN) of the Lambda function to be invoked by the EventBridge rule.",
                "input": " The JSON-encoded payload to be passed to the Lambda function."
            },
            "eventBridgePermissions": {
                "statement_id": "The unique identifier for the permission.",
                "action": "The AWS Lambda action that is being allowed, in this case 'lambda:InvokeFunction' to permit function execution.",
                "function_name": "The name of the Lambda function that is allowed to be invoked by the EventBridge service.",
                "principal": " The service or entity that is allowed to invoke the Lambda function, in this case 'events.amazonaws.com' for EventBridge.",
                "source_arn": "The Amazon Resource Name (ARN) of the EventBridge rule that is allowed to invoke the Lambda function."
            }
        },
        "terraformMainFile": {
            "title": "Terraform Main File",
            "description1": "The ",
            "description2": " file serves as the central configuration for deploying and managing the infrastructure on AWS using Terraform. It orchestrates the setup of the modules and resources previously defined, ensuring that all components are correctly provisioned and interconnected to form a cohesive infrastructure environment."
        },
        "lambdaAggregatorFunction": {
            "title": "Lambda Aggregator Function",
            "description": "The Lambda Aggregator is a Python 3.11 serverless function designed to consolidate multiple CloudTrail log files from a single day into one aggregated file. This function is triggered by EventBridge every 12 hours and performs log consolidation, compression, and cleanup operations to optimize storage and prepare data for analysis.",
            "purposeTitle": "Purpose ",
            "purposes": {
                "consolidateTitle": "Consolidate ",
                "consolidateDescription": "multiple daily CloudTrail log files into a single aggregated file.",
                "reduceTitle": "Reduce ",
                "reduceDescription": "the number of individual files for efficient processing.",
                "cleanupTitle": "Cleanup ",
                "cleanupDescription": "original log files to optimize storage costs.",
                "prepareTitle": "Prepare ",
                "prepareDescription": "data for analysis by the analyzer function."
            },
            "fullCode": "Full Code",
            "functionWorkflow": "Function Workflow",
            "triggerAndInitialization": {
                "title": "1. Trigger and Initialization",
                "description": "The function is triggered by EventBridge every 12 hours and the target bucket is defined."
            },
            "dynamicPath": {
                "title": "2. Dynamic Path Construction",
                "description": "Calculates the current date and creates a dynamic prefix for the log files, in the format of AWSLogs/[AccountID]/CloudTrail/us-east-1/YYYY/MM/DD/"
            },
            "fileDiscovery": {
                "title": "3. File Discovery and Filtering",
                "description": "Lists all objects in the current day's folder, filters for .json.gz files only, and excludes already aggregated files to prevent reprocessing."
            },
            "dataAggregation": {
                "title": "4. Data Aggregation",
                "description": "Downloads each individual log file, decompresses gzip content in memory, extracts Records array from each file, and consolidates all records into a single array."
            },
            "outputFile": {
                "title": "5. Output File Creation",
                "description": "Generates a timestamp-based filename, compresses aggregated data using gzip, and uploads the consolidated file to S3 in the format YYYY-MM-DD-HH:MM-aggregated.json.gz."
            },
            "cleanup": {
                "title": "6. Cleanup Operations",
                "description": "Deletes original individual log files while preserving the new aggregated file, optimizing storage costs and organization."
            },
            "results": {
                "title": "7. Results",
                "description1": "The function completes successfully, consolidating logs and optimizing storage.",
                "description2": "Before the aggregation, the files are as follows:",
                "description3": "After the aggregation, the files are as follows:"
            }
        },
        "lambdaAnalyzerFunction": {
            "title": "Lambda Analyzer Function",
            "description": "The Lambda Analyzer is a Python 3.11 serverless function designed to process aggregated CloudTrail log files and generate comprehensive statistical insights about AWS account activities. This function is triggered automatically by S3 ObjectCreated events when aggregated log files are uploaded, and it produces detailed reports in both JSON and CSV formats for further analysis.",
            "purposeTitle": "Purpose ",
            "purposes": {
                "processTitle": "Process ",
                "processDescription": "aggregated CloudTrail log files to extract meaningful insights.",
                "analyzeTitle": "Analyze ",
                "analyzeDescription": "AWS account activities and usage patterns.",
                "generateTitle": "Generate ",
                "generateDescription": "statistical reports on events, resources, regions, and user activities.",
                "exportTitle": "Export ",
                "exportDescription": "results in multiple formats (JSON and CSV) for dashboard integration."
            },
            "handlerCode": "Handler Code",
            "analyzerCode": "Analyzer Code",
            "exportCode": "Export Code",
            "functionWorkflow": "Function Workflow",
            "trigger": {
                "title": "1. Trigger and S3 Event Processing",
                "description": "The function is triggered automatically by S3 ObjectCreated events when new files are uploaded to the input bucket. It extracts bucket name and object key from the S3 event payload."
            },
            "fileDownload": {
                "title": "2. File Download and Path Setup",
                "description": "Downloads the triggered file from S3 to the Lambda's /tmp directory for local processing, and extracts the base filename for future use."
            },
            "fileDecompression": {
                "title": "3. File Decompression",
                "description": "Decompresses the gzipped log file to extract the JSON content, creating a readable JSON file in the Lambda's temporary storage."
            },
            "fileValidation": {
                "title": "4. Aggregated File Validation and Processing",
                "description": "Validates that the file is an aggregated log file before processing. If valid, calls the analyzer module to process the logs and defines output paths for the insights files."
            },
            "dataAnalysis": {
                "title": "5. Data Analysis and Insights Generation",
                "description": "Processes the JSON log file to extract and count various metrics including event names, resource types, regions, source IPs, event types, event categories, and user identity types."
            },
            "dataExport": {
                "title": "6. Data Export and Format Conversion",
                "description": "Exports the generated insights to both JSON (complete data structure) and CSV (event counts only) formats for different use cases and integrations."
            },
            "output": {
                "title": "7. Output Upload to S3",
                "description": "Uploads the generated insights to the output S3 bucket in the specified paths, completing the analysis process."
            },
            "results": {
                "title": "8. Results",
                "description": "The output files are structured as follows:"
            }
        },
        "dashboard": {
            "title": "Dashboard",
            "description1": "The Dashboard is an interactive visualization interface that transforms the processed AWS account activity data from the Lambda Analyzer into intuitive charts and metrics. This dashboard provides stakeholders with real-time insights into AWS usage patterns, security events, and operational trends through visual representations of event frequencies, regional distribution, user activities, resource utilization, and source IP analysis.",
            "description2": "To see the dashboard created with the processed logs, ",
            "description3": "go to the dashboard page."
            
        },
        "dashboardPage": {
            "title": "Dashboard - 18/07/2025",
            "description1": "This dashboard visualizes real AWS CloudTrail data from a live AWS account, showing ",
            "description2": " captured during the analysis period. The data reveals that the account primarily operates in the",
            "description3": "region with activity dominated by ",
            "description5": "The most frequent operations include",
            "description6": "and ",
            "description7": "indicating active monitoring and security operations.",
            "description8": "generated the majority of activities, followed by ",
            "description9": "with most traffic originating from a specific IP address",
            "description10": "The resource usage shows heavy operations with",
            "description11": "and",
            "description12": "providing insights into storage management and identity operations across the AWS environment.",
            "commonEventsTitle": "Common Events",
            "sourceIPsTitle": "Source IP Counts",
            "resourceCountsTitle": "Resource Counts",
            "activityCountsTitle": "Activity Counts",
            "totalEvents": "total events",
            "events": "events",
            "calls": "calls",
            "operations": "operations"
        }
    }
}
